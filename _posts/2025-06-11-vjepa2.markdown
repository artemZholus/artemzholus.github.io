---
layout: post
title:  "V-JEPA 2: Self-Supervised Video Models Enable Understanding, Prediction and Planning"
date:   2025-06-11 00:00:00 +00:00
year: "2025"
image: /images/vjepa2.png
categories: research
author: "Artem Zholus"
authors: "Mido Assran*, Adrien Bardes*, David Fan*, Quentin Garrido*, Russell Howes*, Mojtaba Komeili*, Matthew Muckley*, Ammar Rizvi*, Claire Roberts*, Koustuv Sinha*, <strong style=\"text-decoration: underline;\">Artem Zholus*</strong>, Sergio Arnaud*, Abha Gejji*, Ada Martin*, Francois Robert Hogan*, Daniel Dugas*, Piotr Bojanowski, Vasil Khalidov, Patrick Labatut, Francisco Massa, Marc Szafraniec, Kapil Krishnakumar, Yong Li, Xiaodong Ma, Sarath Chandar, Franziska Meier*, Yann LeCun*, Michael Rabbat*, and Nicolas Ballas*"
venue: "Technical Report"
website: "https://ai.meta.com/vjepa"
arxiv: "https://arxiv.org/abs/2506.09985"
code: "https://github.com/facebookresearch/vjepa2"
hf: "https://huggingface.co/collections/facebook/v-jepa-2-6841bad8413014e185b497a6"
blogpost: "https://ai.meta.com/blog/v-jepa-2-world-model-benchmarks"
---
By scaling world model pretraining to over a million hours of internet videos, we build V-JEPA 2 that excels at motion understanding, human-action anticipation, and video question answering. We show how action-conditioned post training on just 62 hours of unlabeled robot videos, enables zero-shot generalization in robotic control through planning in the latent space for tasks such as pick-and-place. 